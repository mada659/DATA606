{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSZVf7PATbjQ"
   },
   "source": [
    "### Entropy (Physics)\n",
    "Entropy is often interpreted as the degree of disorder or randomness in the system.\n",
    "\n",
    "How many ways can I get 4 heads (\"order\") if I toss a fair coin 4 times?\n",
    " HHHH --> only one way. Probabilty = 1/16\n",
    "\n",
    "How many ways can I get 2 heads and 2 tails (\"disorder\", \"messy\") if I toss to a fair coin 4 times?\n",
    " HHTT, HTHT, TTHH, THTH, HTTH, THHT --> 6 ways. Probability = 6/16\n",
    "\n",
    " There are more ways to have \"disorder\" than \"order\". The increase of entropy in the universe is statistical,i.e., disorder is more probable than order. It explains the arrow of time (past, present, future). Why do we grow old but not get younger? Why do we remember the past but not the future?\n",
    "\n",
    " Life struggles against entropy to maintain existence but increases the overall entropy of the universe by consuming and wasting energy.\n",
    "\n",
    " I was featured in [The Big Question](https://terp.umd.edu/bigq13/#.YhLhHu7MLMC) for the Terp Mag. My response to the prompt, “How Could Climate Change Affect Your Field?” was, “The energy cost of information needs more public awareness. The carbon footprint of machine learning training for an AI entity can be multiple times greater than the lifetime footprint of a car. The use of information should be managed as a resource with environmental costs, similar to the use of fossil fuels.”\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEpTSbynwO4C"
   },
   "source": [
    "<img src='https://image.slidesharecdn.com/time-pop-11-120918154634-phpapp02/95/the-origin-of-the-universe-and-the-arrow-of-time-20-728.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rTiaFbqUb2m"
   },
   "source": [
    "Is the meaning of life to speed up the entropy (\"disorder\") of the universe?\n",
    "\n",
    "<img src='https://image.slidesharecdn.com/time-pop-11-120918154634-phpapp02/95/the-origin-of-the-universe-and-the-arrow-of-time-16-728.jpg?cb=1347984480'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7S-HGStVPV3"
   },
   "source": [
    "### Entropy (Information Theory)\n",
    "The information content of an event E is defined as ${\\displaystyle I(E)=-\\log _{2}(p(E))}$ or ${\\displaystyle I(E)=\\log _{2}(1/p(E))}$ \n",
    "where $p(E)$ is the probability of the event. The more surprising an event is (low probability), the more info content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOV2OIUaVjia"
   },
   "source": [
    "Claude Shannon (1916 – 2001), mathematician, electrical engineer, cryptographer\n",
    "\n",
    "<img src='https://i.ytimg.com/vi/z2Whj_nL-x8/hqdefault.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRm3P7RGrpip"
   },
   "source": [
    "Edwin Thompson Jaynes (July 5, 1922 – April 30, 1998) was the Wayman Crow Distinguished Professor of Physics at Washington University in St. Louis.\n",
    "\n",
    "1. Jaynes, E.T. (1957). [\"Information theory and statistical mechanics\"](https://bayes.wustl.edu/etj/articles/theory.1.pdf)(PDF). Physical Review. 106 (4): 620–630.\n",
    "\n",
    "2. Edwin Thompson Jaynes. Probability Theory: The Logic of Science. Cambridge University Press, (2003)\n",
    "\n",
    "![image](https://www.azquotes.com/picture-quotes/quote-it-was-our-use-of-probability-theory-as-logic-that-has-enabled-us-to-do-so-easily-what-edwin-thompson-jaynes-70-25-90.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_JnUqAQVlKg"
   },
   "source": [
    "John Wheeler (1911 – 2008) , theoretical physicist\n",
    "\n",
    "<img src='https://i.ytimg.com/vi/6qMi0K6hlYA/hqdefault.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cCJBX-DsRp7Z",
    "outputId": "8088b35e-c720-48e7-cbe6-32ce067015ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fair coin\n",
    "from math import log2\n",
    "p = 0.5\n",
    "# info content of heads is one bit\n",
    "I = -log2(p)\n",
    "I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2gOIGSemMqc"
   },
   "source": [
    "Let Heads  = 1 and Tails = 0. Then each side can be represented by one bit.\n",
    "\n",
    "` 1 ` \n",
    "\n",
    "` 0 `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djIAirS3mdrS",
    "outputId": "4166b740-59ee-40cb-bd81-fb1b81127c13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.584962500721156"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fair die\n",
    "p = 1/6\n",
    "# info content of rolling 5 is 2.858 bits\n",
    "I = -log2(p)\n",
    "I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP9ltjcrmi77"
   },
   "source": [
    "Each side of the die can be represented by 3 bits:\n",
    "\n",
    "` 0 0 1  `   ----> 1     \n",
    "` 0 1 0  `   -----> 2  \n",
    "` 0 1 1  `   ------>3    \n",
    "` 1 0 0  `  ------>4    \n",
    "` 1 0 1  `    ---->5   \n",
    "` 1 1 0  `    ----->6\n",
    "\n",
    "  Shannon's formula says we can do better, in principle. Each side of the die can be represented by 2.58 bits. Obviously, we haven't used `000` and `111` in the 3 bit representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xrPTKxCn6uT"
   },
   "source": [
    "Information theory defines the entropy $H$ of a random variable as the average info content of the variable's possible outcomes. \n",
    "$H(X)=E(I(X))=E(-\\log(P(X))$, \n",
    "where $\\operatorname {E}$ is the expected value or average of a variable.\n",
    "For a discrete random variable:\n",
    "${\\displaystyle \\mathrm {H} (X)=-\\sum _{i=1}^{n}{\\mathrm {P} (x_{i})\\log _{2}\\mathrm {P} (x_{i})}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rpRF3UgGoBci",
    "outputId": "3f05fe07-9266-4219-ce43-449246b4c32c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.70043971814109"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a fair English alphabet\n",
    "p=1/26\n",
    "H = -sum([p * log2(p) for _ in range(26)])\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmtVrvAroHg7"
   },
   "source": [
    "### Intuition: \n",
    "The quantity of information is the number of bits required to send a random event from a probability distribution. It is also considered to measure \"Surprise\".  An event with high probability has less surprise, less entropy, less info, e.g., the Sun rose from the east this morning.\n",
    "An event with low probability surprises us; it has more info, more entropy, e.g., Breaking News: The College decided to ban all homework. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l72Zrq4naOf0"
   },
   "source": [
    "The number of Yes and No questions--->Binary Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9sIvPLvk_-1"
   },
   "source": [
    "## Huffman Coding Algorithm\n",
    "\n",
    "In 1951, David A. Huffman and his MIT information theory classmates were given the choice of a term paper or a final exam. The professor, Robert M. Fano, assigned a term paper on the problem of finding the most efficient binary code. Huffman, unable to prove any codes were the most efficient, was about to give up and start studying for the final when he hit upon the idea of using a frequency-sorted binary tree and quickly proved this method the most efficient.\n",
    "\n",
    "In doing so, Huffman outdid Fano, who had worked with information theory inventor Claude Shannon to develop a similar code. Building the tree from the bottom up guaranteed optimality, unlike the top-down approach of Shannon–Fano coding.\n",
    "\n",
    "[Reference](https://en.wikipedia.org/wiki/Huffman_coding)\n",
    "\n",
    "\n",
    "![image](https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Huffman_coding_example.svg/2560px-Huffman_coding_example.svg.png)\n",
    "\n",
    "A source generates 4 different symbols ${\\displaystyle \\{a_{1},a_{2},a_{3},a_{4}\\}}$ with probability ${\\displaystyle \\{0.4;0.35;0.2;0.05\\}}$. A binary tree is generated from left to right taking the two least probable symbols and putting them together to form another equivalent symbol having a probability that equals the sum of the two symbols. The process is repeated until there is just one symbol. The tree can then be read backwards, from right to left, assigning different bits to different branches. The final Huffman code is:\n",
    "\n",
    "Symbol\tCode\n",
    "\n",
    "a1\t0\n",
    "\n",
    "a2\t10\n",
    "\n",
    "a3\t110\n",
    "\n",
    "a4\t111\n",
    "\n",
    "The standard way to represent a signal made of 4 symbols is by using 2 bits/symbol, but the entropy of the source is 1.74 bits/symbol. If this Huffman code is used to represent the signal, then the average length is lowered to 1.85 bits/symbol; it is still far from the theoretical limit because the probabilities of the symbols are different from negative powers of two.\n",
    "\n",
    "## Example\n",
    "\n",
    "P(A) = 1/2, P(B) = 1/4, P(C) = 1/4\n",
    "Is it A? Yes or No. If No, then is it B? Yes or No\n",
    "\n",
    "Entropy H = 1.5\n",
    " \n",
    "        Node\n",
    "        / \\\n",
    "       A.  Node\n",
    "           / \\\n",
    "          B.  C\n",
    "\n",
    "A-->1 \n",
    "\n",
    "B-->01 \n",
    "\n",
    "C-->00\n",
    "\n",
    "Length of Huffman code: (1/2)*1 + (1/4)*2 + (1/4)*2 = 1.5\n",
    "\n",
    "Another coding\n",
    "\n",
    "        Node\n",
    "        / \\\n",
    "       B.  Node\n",
    "           / \\\n",
    "          A.  C\n",
    "B-->1 \n",
    "\n",
    "A-->01 \n",
    "\n",
    "C-->00\n",
    "\n",
    "Length of coding = (1/4)*1 + (1/2)*2 + (1/4)*2 = 1.75 \n",
    "\n",
    "Huffman's Paper:\n",
    "[A Method for the Construction of\n",
    "Minimum-Redundancy Codes](http://compression.ru/download/articles/huff/huffman_1952_minimum-redundancy-codes.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mb7LsTgQO_IL"
   },
   "source": [
    "### Using the library scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SiO8F47YPEJ6",
    "outputId": "9213c630-dd94-415a-a01c-fc43f5203e1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.584962500721156"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fair die\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "p = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]\n",
    "# Entropy\n",
    "H = entropy(p, base=2)\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FygabFtPU8c"
   },
   "source": [
    "### Cross-Entropy \n",
    "Average number of total bits to represent an event from distribution q instead of distribution P.\n",
    "\n",
    "It is used as a loss function in  Machine Learning, e.g., Logistic Regression\n",
    "\n",
    "The cross-entropy of the distribution ${\\displaystyle q}$ relative to a distribution ${\\displaystyle p}$ over a given set is defined as follows:\n",
    "\n",
    "${\\displaystyle H(p,q)=-\\operatorname {E} _{p}[\\log q]}$\n",
    "\n",
    "For discrete probability distributions:\n",
    "\n",
    "${\\displaystyle H(p,q)=-\\sum _{x\\in {\\mathcal {X}}}p(x)\\,\\log q(x)}$\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajYCJrd1PbvG",
    "outputId": "2a8e4e59-79e9-4efa-815b-c8cd756d7507"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.736965594166206"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log2\n",
    "\n",
    "def CrossEntropy(p, q):\n",
    "\treturn -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
    " \n",
    "p = [1/2, 1/2]\n",
    "q = [9/10, 1/10]\n",
    " \n",
    "CrossEntropy(p, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oyHaiJhQnUE"
   },
   "source": [
    "## Relative entropy or Kullback-Leibler divergence\n",
    "Average number of extra bits to represent an event from distribution q instead of distribution p.\n",
    "\n",
    "In the context of machine learning, ${\\displaystyle D_{\\text{KL}}(P\\parallel Q)}$ is often called the information gain achieved if ${\\displaystyle P}$ would be used instead of ${\\displaystyle Q}$ which is currently used. \n",
    "\n",
    "In Bayesian inference: ${\\displaystyle D_{\\text{KL}}(P\\parallel Q)}$ is a measure of the information gained by revising one's beliefs from the prior probability distribution ${\\displaystyle Q}$ to the posterior probability distribution ${\\displaystyle P}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oG6PArCSMlV",
    "outputId": "b543d637-5ff3-47b6-c93a-c9dc7f7c90e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7369655941662062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18872187554086714"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(entropy([1/2, 1/2], qk=[9/10, 1/10], base=2))\n",
    "entropy([3/4, 1/4], qk=[1/2, 1/2], base=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IInOzBw0Sm_f"
   },
   "source": [
    "## Cross-Entropy = Entropy + KL-divergence\n",
    "\n",
    "\n",
    "${\\displaystyle H(p,q)=H(p)+D_{\\mathrm {KL} }(p\\|q)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WkOgc5ThFdj"
   },
   "source": [
    "## Mutual Information\n",
    "\n",
    "For jointly discrete or jointly continuous pairs ${\\displaystyle (X,Y)}$, mutual information is the Kullback–Leibler divergence from the product of the marginal distributions, ${\\displaystyle p_{X}\\cdot p_{Y}}$, of the joint distribution ${\\displaystyle p_{(X,Y)}}{\\displaystyle p_{(X,Y)}}$, that is,\n",
    "\n",
    "${\\displaystyle \\operatorname {I} (X;Y)=D_{\\text{KL}}\\left(p_{(X,Y)}\\parallel p_{X}p_{Y}\\right)}$\n",
    "\n",
    "Independence ==> no correlation but the converse is not true\n",
    "\n",
    "Mutual info captures both linear and non-linear dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2W605rKJb2pO"
   },
   "source": [
    "## Algorithmic Entropy\n",
    "\n",
    "The Kolmogorov complexity of an object, such as a piece of text, is the length of a shortest computer program (in a predetermined programming language) that produces the object as output.\n",
    "\n",
    "Consider the following two strings of 32 lowercase letters and digits:\n",
    "\n",
    "abababababababababababababababab , and\n",
    "\n",
    "4c1j5b2p0cv4w1x8rx2y39umgw5q85s7\n",
    "\n",
    "The first string has a short English-language description, namely \"write ab 16 times\", which consists of 17 characters. The second one has no obvious simple description (using the same character set) other than writing down the string itself, i.e., \"write 4c1j5b2p0cv4w1x8rx2y39umgw5q85s7\" which has 38 characters. Hence the operation of writing the first string can be said to have \"less complexity\" than writing the second.\n",
    "\n",
    "[Reference](https://en.wikipedia.org/wiki/Kolmogorov_complexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJZjjIU4hjO9"
   },
   "source": [
    "## Algorithmic Entropy as a Measure of Compression\n",
    "\n",
    "The compression of data (e.g., Zip file), a scientific rule (e.g., Newton's laws), a Machine Learning model, Encoder/Decoder (e.g., GANs) are algorithms that capture the essence of the data and reduce redundancy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TzIbuOVvesJ2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7393538721672006\n",
      "1.85\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'no' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_229/245353739.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# (3) Does the result of (2) match the result of (1)?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# (4) Can we design an algorithm so that the result of (2) is smaller than the result of (1) ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'no' is not defined"
     ]
    }
   ],
   "source": [
    "# Exercise:\n",
    "from math import log2\n",
    "# (1) What is the entropy of the probability distribution for the above example with a1, a2, a3, a4 ?\n",
    "prob = [0.4, 0.35, 0.2, 0.05]\n",
    "H = -sum([prob[i] * log2(prob[i]) for i in range(len(prob))])\n",
    "print(H)\n",
    "\n",
    "# (2) What is the average codebits per symbol for the Huffman coding of a1, a2, a3, a4?\n",
    "print(0.4*(1)+0.35*(2)+0.2*(3)+0.05*(3))\n",
    "\n",
    "# (3) Does the result of (2) match the result of (1)?\n",
    "no\n",
    "\n",
    "# (4) Can we design an algorithm so that the result of (2) is smaller than the result of (1) ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.739353872167201\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "H= entropy(prob, base=2)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Information_Theory.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
