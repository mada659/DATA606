{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eHkK2tFxvfc"
   },
   "source": [
    "### Fitness Landscapes\n",
    "\n",
    "An evolving population typically climbs uphill in the fitness landscape, by a series of small genetic changes, until – in the infinite time limit – a local optimum is reached\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Fitness_landscape)\n",
    "\n",
    "![image](https://upload.wikimedia.org/wikipedia/commons/f/fe/Visualization_of_a_population_evolving_in_a_static_fitness_landscape.gif)\n",
    "\n",
    "![image](https://upload.wikimedia.org/wikipedia/commons/a/af/Visualization_of_a_population_evolving_in_a_dynamic_fitness_landscape.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RM3244mZ-WOT"
   },
   "source": [
    "## Optimization\n",
    "\n",
    "Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criterion, from some set of available alternatives.Optimization problems of sorts arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.\n",
    "\n",
    "In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function.\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Mathematical_optimization)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tunokqJU_OAQ"
   },
   "source": [
    "\n",
    "## Optimization problems\n",
    "\n",
    "[Nestorov](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.855&rep=rep1&type=pdf)\n",
    "\n",
    "\"... despite to their attraction, the proposed\n",
    "“solutions” of general optimization problems very often can break down the expectations of\n",
    "a naive user. The main fact, which should be known to any person dealing with optimization\n",
    "models, is that, in general, the optimization problems are unsolvable. In our opinion, this\n",
    "statement, which is usually missed in standard optimization courses, is very important for\n",
    "understanding the entire optimization theory, its past and its future.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vc69hixnFy3o"
   },
   "source": [
    "### References\n",
    "\n",
    "1. Introductory Lectures on Convex Programming Volume I: Basic course \n",
    "   [Nestorov](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.855&rep=rep1&type=pdf)\n",
    "\n",
    "2. Convex Optimization – Boyd and Vandenberghe [Boyd](https://web.stanford.edu/~boyd/cvxbook/)\n",
    "\n",
    "3. Algorithms for Optimization By Mykel J. Kochenderfer and Tim A. Wheeler\n",
    "   [Kochenderfer](https://mitpress.mit.edu/books/algorithms-optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZBbOVErJErK"
   },
   "source": [
    "## Basic Optimization Problem\n",
    "\n",
    "$argmin_x f(x)$\n",
    "\n",
    "$x\\in X$\n",
    "\n",
    "We call $f$ the objective function and $X$ the feasible set\n",
    "\n",
    "Note that $argmin_x f(x) = argmax_x -f(x)$\n",
    "\n",
    "\n",
    "[Slides](https://web.stanford.edu/~mykel/algforopt/slides/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IPqVIcKj1Vs"
   },
   "source": [
    "## Constraints\n",
    "\n",
    "Constraints define the feasible set. They are written with <=, >=, or =.\n",
    "\n",
    "For example: \n",
    "\n",
    "$ x_1 >= 0$, \n",
    "\n",
    "$x_2 >=0$, \n",
    "\n",
    "$2*x_1 + x_2 <= 2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "xz3alZopxsHD",
    "outputId": "f62eea69-fc8e-45e4-dd04-9d17a60a757d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x_1 = np.linspace(0, 10, 100)\n",
    "x_2 = np.linspace(0, 10, 100)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "# constraints\n",
    "plt.axvline(0, color='g', label=r'$x_2 \\geq 0$') \n",
    "plt.axhline(0, color='r', label=r'$x_1 \\geq 0$')  \n",
    "plt.plot(x_1, 2 - (2*x_1), label=r'$2x_1 + x_2 \\leq 2$') \n",
    "\n",
    "\n",
    "plt.xlim((0, 3))\n",
    "plt.ylim((0, 3))\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "\n",
    "# Feasible region\n",
    "plt.fill_between(x_1, 2 - (2*x_1), color='green')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJ3RJPL96MA4"
   },
   "source": [
    "## Critical Points\n",
    "\n",
    "If the objective function $f$ is differentiable, then the points where $f$ has a derivative of zero are called  the Critical Points. This is a necessary but not a sufficient condition for maxima or minima. For example, inflection points are critical points but neither maxima nor minima.\n",
    "\n",
    "![image](https://i.ytimg.com/vi/AY0MUskpaHQ/hqdefault.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXWQUwJBGFkV"
   },
   "source": [
    "## Conditions for local Minima\n",
    "\n",
    "If $x^\\star$ is a local minimum, then a necessary condition is $f'(x^\\star) = 0$ (first-order condition)\n",
    "\n",
    "If $x^\\star$ is a local minimum, then a necessary condition is $f''(x^\\star) \\geq 0$ (second-order condition)\n",
    "\n",
    "If $f'(x^\\star) = 0$ and $f''(x^\\star) > 0$, then $x^\\star$ is a (strong) local minimum\n",
    "\n",
    "An inflection point or a weak local minimum satisfies $f''(x^\\star) \\geq 0$\n",
    "\n",
    "\n",
    "\n",
    "![image](https://scpo-compecon.github.io/CoursePack/assets/figs/optimization/critical-points.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHXOK00sjUxz",
    "tags": []
   },
   "source": [
    "## Multivariate functions\n",
    "\n",
    "If $x^\\star$ is a local minimum, then a necessary condition is $\\nabla f(x^\\star) = 0$ (first-order condition)\n",
    "\n",
    "If $x^\\star$ is a local minimum, then a necessary condition is $\\nabla^2 f(x^\\star)$ is positive semidefinite (second-order condition).\n",
    "\n",
    "\n",
    "If $\\nabla f(x^\\star) = 0$ and $\\nabla^2 f(x^\\star)$ is positive definite, then $x^\\star$ is a (strong) local minimum \n",
    "\n",
    "A symmetric matrix M is positive semidefinite if $x^TMx \\geq 0$ and positive definite if $x^TMx > 0$ for $x\\neq 0$\n",
    "\n",
    "A symmetric matrix M is positive definite iff all its eigenvalues are real and positive.\n",
    "\n",
    "![image](https://media.geeksforgeeks.org/wp-content/uploads/20200609231350/surface-21.png). ![image](https://support.minitab.com/en-us/minitab/18/surface_plot_wire_def.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**added** \n",
    "cost function for regression -> MSE -> convex -> easy for Gradient descent\n",
    "cost function for logistic regression --> cross-entropy loss\n",
    "\n",
    "Nash equilibrium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f54O2B5Z3_gF"
   },
   "source": [
    "## Optimization for Neural Networks\n",
    "\n",
    "Saddle points are more likely than local minima in higher dimensions. Why?\n",
    "\n",
    "![imgae](https://miro.medium.com/max/600/1*favAf6-bxBs5qo_Vv0Wn4Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wh3bPRXMDy1R"
   },
   "source": [
    "## Example\n",
    "\n",
    "Let $f(x_1,x_2) = (1-x_1)^2 + 5(x_2 - x_1^2)^2$\n",
    "\n",
    "We want to check if $(x_1,x_2)=(1,1)$ satifies the first-order condition, i.e., $\\nabla f(1,1) = 0$\n",
    "\n",
    "$\\nabla f(x_1,x_2) =  [\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}] =[-2(1-x_1) - (5)(2)(x_2 -x_1^2)(-2x_1) , \\ (5)(2)(x_2 - x_1^2)]$\n",
    "\n",
    "It follows that $\\nabla f(1,1) = 0$\n",
    "\n",
    "Next we check if the Hessian, $\\nabla^2 f(x_1,x_2)$, is positive definite at the point $(1,1)$\n",
    "\n",
    "$\\nabla^2 f(x_1,x_2) = [[\\frac{\\partial f}{\\partial x_1\\ \\partial x_1}, \\frac{\\partial f}{\\partial x_1 \\partial x_2}], [\\frac{\\partial f}{\\partial x_2\\ \\partial x_1}, \\frac{\\partial f}{\\partial x_2 \\partial x_2}]] = [[-20x_2 + 20x_1^2 + 40x_1^2 +2, \\ - 20x_1], [-20x_1, 10]]$\n",
    "\n",
    "It follows that $\\nabla^2 f(1,1) = [[42, -20],[-20,10]]$. The matrix is positive definite, therefore (1,1) is a minimum.\n",
    "\n",
    "The Hessian is the Jacobian of the gradient. How?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYCbfp2ze-A1"
   },
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "The derivatives of a function can be computed by an algorithm using automatic differentiation. It is based on the chain rule:\n",
    "\n",
    "$\\frac{d f(g(x))}{d x} = \\frac{d f}{d g} \\frac{d g}{d x}$\n",
    "\n",
    "We use computaional graphs for automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvqXTKK7iTkh"
   },
   "source": [
    "## Computational Graphs\n",
    "\n",
    "Deep Learning frameworks like Tensorflow and Pytorch use computational graphs and support autodiff. \n",
    "\n",
    "We use forward pass and backward pass on the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "4Z_09EqgpV9L",
    "outputId": "9c498591-9807-4262-e81c-474d5a13ab61"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: pet&#45;shop Pages: 1 -->\n",
       "<svg width=\"400pt\" height=\"77pt\"\n",
       " viewBox=\"0.00 0.00 399.88 77.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 73)\">\n",
       "<title>pet&#45;shop</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-73 395.8786,-73 395.8786,4 -4,4\"/>\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title></title>\n",
       "</g>\n",
       "<!-- * -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>*</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"245.9129,-27.0493 247.6889,-27.1479 249.4466,-27.2953 251.1788,-27.4913 252.8784,-27.7353 254.5386,-28.0266 256.1529,-28.3645 257.7149,-28.7479 259.2189,-29.1759 260.6591,-29.6472 262.0304,-30.1606 263.328,-30.7147 264.5475,-31.308 265.6853,-31.9388 266.7377,-32.6054 267.702,-33.3059 268.5758,-34.0385 269.3572,-34.8012 270.0449,-35.5918 270.6379,-36.4082 271.1358,-37.2481 271.5389,-38.1093 271.8475,-38.9894 272.0628,-39.886 272.1861,-40.7965 272.2193,-41.7186 272.1645,-42.6497 272.0243,-43.5873 271.8014,-44.5287 271.499,-45.4713 271.1203,-46.4127 270.6689,-47.3503 270.1485,-48.2814 269.5627,-49.2035 268.9156,-50.114 268.211,-51.0106 267.4529,-51.8907 266.6451,-52.7519 265.7916,-53.5918 264.8962,-54.4082 263.9626,-55.1988 262.9943,-55.9615 261.9947,-56.6941 260.9672,-57.3946 259.9147,-58.0612 258.8402,-58.692 257.7464,-59.2853 256.6357,-59.8394 255.5104,-60.3528 254.3727,-60.8241 253.2242,-61.2521 252.0669,-61.6355 250.9021,-61.9734 249.7312,-62.2647 248.5554,-62.5087 247.3758,-62.7047 246.1933,-62.8521 245.0089,-62.9507 243.8233,-63 242.6373,-63 241.4517,-62.9507 240.2673,-62.8521 239.0849,-62.7047 237.9053,-62.5087 236.7295,-62.2647 235.5586,-61.9734 234.3938,-61.6355 233.2364,-61.2521 232.088,-60.8241 230.9502,-60.3528 229.825,-59.8394 228.7143,-59.2853 227.6204,-58.692 226.5459,-58.0612 225.4935,-57.3946 224.4659,-56.6941 223.4664,-55.9615 222.4981,-55.1988 221.5644,-54.4082 220.669,-53.5918 219.8155,-52.7519 219.0078,-51.8907 218.2497,-51.0106 217.545,-50.114 216.8979,-49.2035 216.3122,-48.2814 215.7917,-47.3503 215.3403,-46.4127 214.9617,-45.4713 214.6592,-44.5287 214.4364,-43.5873 214.2961,-42.6497 214.2414,-41.7186 214.2745,-40.7965 214.3979,-39.886 214.6131,-38.9894 214.9218,-38.1093 215.3248,-37.2481 215.8228,-36.4082 216.4158,-35.5918 217.1034,-34.8012 217.8848,-34.0385 218.7586,-33.3059 219.7229,-32.6054 220.7754,-31.9388 221.9131,-31.308 223.1327,-30.7147 224.4303,-30.1606 225.8016,-29.6472 227.2418,-29.1759 228.7457,-28.7479 230.3078,-28.3645 231.9221,-28.0266 233.5823,-27.7353 235.2819,-27.4913 237.014,-27.2953 238.7717,-27.1479 240.5478,-27.0493 242.3349,-27 244.1257,-27 245.9129,-27.0493\"/>\n",
       "<text text-anchor=\"middle\" x=\"243.2303\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">*</text>\n",
       "</g>\n",
       "<!-- &#45;&gt;* -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>&#45;&gt;*</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M54.4765,-48.3887C60.2345,-49.4755 66.2915,-50.4347 72,-51 127.0582,-56.4518 141.2721,-54.5292 196.4869,-51 199.7529,-50.7912 203.1395,-50.5012 206.5293,-50.1605\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"207.0501,-53.6239 216.5905,-49.0164 206.2591,-46.6687 207.0501,-53.6239\"/>\n",
       "<text text-anchor=\"middle\" x=\"141.7434\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x_3</text>\n",
       "</g>\n",
       "<!-- + -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>+</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"144.426,-.0493 146.202,-.1479 147.9597,-.2953 149.6919,-.4913 151.3915,-.7353 153.0517,-1.0266 154.666,-1.3645 156.2281,-1.7479 157.732,-2.1759 159.1722,-2.6472 160.5435,-3.1606 161.8411,-3.7147 163.0607,-4.308 164.1984,-4.9388 165.2508,-5.6054 166.2152,-6.3059 167.089,-7.0385 167.8704,-7.8012 168.558,-8.5918 169.151,-9.4082 169.6489,-10.2481 170.052,-11.1093 170.3606,-11.9894 170.5759,-12.886 170.6992,-13.7965 170.7324,-14.7186 170.6776,-15.6497 170.5374,-16.5873 170.3145,-17.5287 170.0121,-18.4713 169.6334,-19.4127 169.182,-20.3503 168.6616,-21.2814 168.0759,-22.2035 167.4287,-23.114 166.7241,-24.0106 165.966,-24.8907 165.1582,-25.7519 164.3048,-26.5918 163.4093,-27.4082 162.4757,-28.1988 161.5074,-28.9615 160.5078,-29.6941 159.4803,-30.3946 158.4278,-31.0612 157.3533,-31.692 156.2595,-32.2853 155.1488,-32.8394 154.0235,-33.3528 152.8858,-33.8241 151.7373,-34.2521 150.58,-34.6355 149.4152,-34.9734 148.2443,-35.2647 147.0685,-35.5087 145.8889,-35.7047 144.7065,-35.8521 143.522,-35.9507 142.3364,-36 141.1505,-36 139.9649,-35.9507 138.7804,-35.8521 137.598,-35.7047 136.4184,-35.5087 135.2426,-35.2647 134.0717,-34.9734 132.9069,-34.6355 131.7495,-34.2521 130.6011,-33.8241 129.4633,-33.3528 128.3381,-32.8394 127.2274,-32.2853 126.1335,-31.692 125.0591,-31.0612 124.0066,-30.3946 122.9791,-29.6941 121.9795,-28.9615 121.0112,-28.1988 120.0775,-27.4082 119.1821,-26.5918 118.3286,-25.7519 117.5209,-24.8907 116.7628,-24.0106 116.0582,-23.114 115.411,-22.2035 114.8253,-21.2814 114.3048,-20.3503 113.8534,-19.4127 113.4748,-18.4713 113.1724,-17.5287 112.9495,-16.5873 112.8093,-15.6497 112.7545,-14.7186 112.7877,-13.7965 112.911,-12.886 113.1263,-11.9894 113.4349,-11.1093 113.8379,-10.2481 114.3359,-9.4082 114.9289,-8.5918 115.6165,-7.8012 116.3979,-7.0385 117.2717,-6.3059 118.2361,-5.6054 119.2885,-4.9388 120.4262,-4.308 121.6458,-3.7147 122.9434,-3.1606 124.3147,-2.6472 125.7549,-2.1759 127.2588,-1.7479 128.8209,-1.3645 130.4352,-1.0266 132.0954,-.7353 133.795,-.4913 135.5271,-.2953 137.2848,-.1479 139.0609,-.0493 140.848,0 142.6389,0 144.426,-.0493\"/>\n",
       "<text text-anchor=\"middle\" x=\"141.7434\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">+</text>\n",
       "</g>\n",
       "<!-- &#45;&gt;+ -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>&#45;&gt;+</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M54.1926,-36.3123C69.6691,-33.0752 89.2417,-28.9814 105.954,-25.4858\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"106.8564,-28.8729 115.928,-23.3996 105.4232,-22.0212 106.8564,-28.8729\"/>\n",
       "<text text-anchor=\"middle\" x=\"83.5\" y=\"-35.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x_1</text>\n",
       "</g>\n",
       "<!-- &#45;&gt;+ -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>&#45;&gt;+</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M45.5426,-23.6987C53.1078,-17.4557 62.3449,-11.2374 72,-8 82.3707,-4.5227 94.006,-4.6948 104.757,-6.391\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"104.2271,-9.8548 114.7236,-8.415 105.6202,-2.9948 104.2271,-9.8548\"/>\n",
       "<text text-anchor=\"middle\" x=\"83.5\" y=\"-11.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x_2</text>\n",
       "</g>\n",
       "<!-- Output -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>Output</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"356.0726,-27.0493 358.4868,-27.1479 360.876,-27.2953 363.2306,-27.4913 365.5408,-27.7353 367.7976,-28.0266 369.9919,-28.3645 372.1152,-28.7479 374.1595,-29.1759 376.1172,-29.6472 377.9812,-30.1606 379.745,-30.7147 381.4028,-31.308 382.9493,-31.9388 384.3799,-32.6054 385.6907,-33.3059 386.8785,-34.0385 387.9407,-34.8012 388.8754,-35.5918 389.6814,-36.4082 390.3583,-37.2481 390.9061,-38.1093 391.3257,-38.9894 391.6183,-39.886 391.786,-40.7965 391.8311,-41.7186 391.7566,-42.6497 391.566,-43.5873 391.263,-44.5287 390.8519,-45.4713 390.3372,-46.4127 389.7236,-47.3503 389.0162,-48.2814 388.22,-49.2035 387.3403,-50.114 386.3826,-51.0106 385.352,-51.8907 384.2541,-52.7519 383.0939,-53.5918 381.8768,-54.4082 380.6077,-55.1988 379.2914,-55.9615 377.9327,-56.6941 376.536,-57.3946 375.1054,-58.0612 373.6448,-58.692 372.158,-59.2853 370.6482,-59.8394 369.1186,-60.3528 367.572,-60.8241 366.011,-61.2521 364.4378,-61.6355 362.8544,-61.9734 361.2628,-62.2647 359.6646,-62.5087 358.0612,-62.7047 356.4538,-62.8521 354.8438,-62.9507 353.2322,-63 351.6201,-63 350.0085,-62.9507 348.3985,-62.8521 346.7912,-62.7047 345.1878,-62.5087 343.5895,-62.2647 341.9979,-61.9734 340.4146,-61.6355 338.8414,-61.2521 337.2804,-60.8241 335.7338,-60.3528 334.2042,-59.8394 332.6944,-59.2853 331.2076,-58.692 329.747,-58.0612 328.3164,-57.3946 326.9196,-56.6941 325.5609,-55.9615 324.2447,-55.1988 322.9756,-54.4082 321.7584,-53.5918 320.5983,-52.7519 319.5003,-51.8907 318.4698,-51.0106 317.512,-50.114 316.6324,-49.2035 315.8362,-48.2814 315.1287,-47.3503 314.5151,-46.4127 314.0004,-45.4713 313.5893,-44.5287 313.2864,-43.5873 313.0958,-42.6497 313.0213,-41.7186 313.0664,-40.7965 313.234,-39.886 313.5267,-38.9894 313.9462,-38.1093 314.4941,-37.2481 315.1709,-36.4082 315.977,-35.5918 316.9117,-34.8012 317.9739,-34.0385 319.1616,-33.3059 320.4724,-32.6054 321.9031,-31.9388 323.4496,-31.308 325.1073,-30.7147 326.8712,-30.1606 328.7352,-29.6472 330.6928,-29.1759 332.7371,-28.7479 334.8605,-28.3645 337.0548,-28.0266 339.3115,-27.7353 341.6218,-27.4913 343.9763,-27.2953 346.3656,-27.1479 348.7798,-27.0493 351.209,-27 353.6433,-27 356.0726,-27.0493\"/>\n",
       "<text text-anchor=\"middle\" x=\"352.4262\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Output</text>\n",
       "</g>\n",
       "<!-- *&#45;&gt;Output -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>*&#45;&gt;Output</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M271.9203,-45C281.5931,-45 292.6784,-45 303.4668,-45\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"303.6636,-48.5001 313.6636,-45 303.6636,-41.5001 303.6636,-48.5001\"/>\n",
       "<text text-anchor=\"middle\" x=\"292.4738\" y=\"-48.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">f</text>\n",
       "</g>\n",
       "<!-- +&#45;&gt;* -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>+&#45;&gt;*</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M166.3106,-24.5359C177.9973,-27.6451 192.2349,-31.433 205.269,-34.9006\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"204.3758,-38.2847 214.9395,-37.4734 206.1755,-31.52 204.3758,-38.2847\"/>\n",
       "<text text-anchor=\"middle\" x=\"192.4869\" y=\"-35.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">g</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fa026e5dc10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz  \n",
    "cg = graphviz.Digraph('pet-shop', node_attr={'shape': 'plaintext'})\n",
    "cg.attr(rankdir='LR') \n",
    "cg.edge('', '*', label ='x_3')\n",
    "cg.node('*', shape='egg') \n",
    "cg.edge('', '+', label ='x_1') \n",
    "cg.edge('', '+', label ='x_2') \n",
    "cg.node('+', shape='egg') \n",
    "cg.edge('+', '*', label ='g')\n",
    "cg.node('Output', shape='egg') \n",
    "cg.edge('*', 'Output', label ='f')\n",
    "cg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUC4OZSJwX44"
   },
   "source": [
    "We start with $x_1 = 5$, $x_2=7$, $x_3=2$ to trigger the forward pass. Then $g=x_1 + x_2 = 12$. Next we compute $f=g*x_3=24$. The output is 21. \n",
    "\n",
    "Now we do the backward pass to compute the gradient. What is $\\nabla f(x_1,x_2,x_3)$ :\n",
    "\n",
    "$\\frac{\\partial f}{∂ x_3} = g$ , since $f=g*x_3$, therefore $\\frac{\\partial f}{∂ x_3} =12$\n",
    "\n",
    "$\\frac{\\partial f}{∂ x_2} = \\frac{\\partial f}{∂ g} \\frac{\\partial g}{∂ x_2}$, according to the chain rule\n",
    "\n",
    "$\\frac{\\partial f}{∂ g} = x_3$, since since $f=g*x_3$, therefore $\\frac{\\partial f}{∂ g} =2 $. Also, $\\frac{\\partial g}{∂ x_2} = 1$, since $g = x_1 + x_2$\n",
    "\n",
    "$\\frac{\\partial f}{∂ x_2} = \\frac{\\partial f}{∂ g} \\frac{\\partial g}{∂ x_2} = (2)(1) = 2$\n",
    "\n",
    "Finally, $\\frac{\\partial f}{∂ x_2} = \\frac{\\partial f}{∂ g} \\frac{\\partial g}{∂ x_2} = (2)(1) =2$\n",
    "\n",
    "Did it occur to you now that the backpropogation of neural networks is just a bottom-up dynamic programming approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6VH1_Elgsoi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "x_1 = torch.tensor(5.).requires_grad_(True)\n",
    "x_2 = torch.tensor(7.).requires_grad_(True)\n",
    "x_3 = torch.tensor(2.).requires_grad_(True)\n",
    "f = (x_1 + x_2)*x_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mD46kjgxjXjw"
   },
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWOEZF4ljdTo",
    "outputId": "cd0cc459-d746-4c89-8642-a9af289d3571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x_1,x_2,x_3)=24.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"f(x_1,x_2,x_3)={f:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yt2s37ciFZk",
    "outputId": "5c240cba-e75e-4e84-e4b1-d0822a098e78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.), tensor(2.), tensor(12.))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.grad, x_2.grad, x_3.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-hmOSfkMGKJ"
   },
   "source": [
    "## Convexity\n",
    "\n",
    "[Reference](https://www.stat.cmu.edu/~ryantibs/convexopt/lectures/convex-fns.pdf)\n",
    "\n",
    "![image](https://miro.medium.com/max/558/1*aev61Yc9n5NoSpxtaMI2dA.png)\n",
    "\n",
    "![imag](https://miro.medium.com/max/1116/1*HEZQ3b4MU0Hpn2lx1PgPZw.png)\n",
    "\n",
    "![image](https://miro.medium.com/max/1116/1*Wk24llrQ4Iu9y-6x30reWQ.png)\n",
    "\n",
    "![image](https://i.imgur.com/PNi83lL.png)\n",
    "\n",
    "![image](https://miro.medium.com/max/1116/1*DlQKPc7_NO2DY_Cis4BpbA.png)\n",
    "\n",
    "### Examples for one variable:\n",
    "\n",
    "$f(x)=x^{2} $ and $f(x)=e^{x}$ are convex functions\n",
    "\n",
    "### Examples of many variables:\n",
    "\n",
    "$f(x)=a^{T}x+b $ is convex and concave\n",
    "\n",
    "${\\displaystyle x^{\\mathsf {T}}A\\,x+b^{\\mathsf {T}}x\\;}$ is convex if $A$ is positive semidefinite\n",
    "\n",
    "#### Exercise: \n",
    "show that $f(x)=|x|$ is convex but not strictly convex\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dac9vGn8-0tE"
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "\n",
    "${\\displaystyle Y_{i}=\\beta _{0}+\\beta _{1}X_{i1}+\\beta _{2}X_{i2}+\\ldots +\\beta _{p}X_{ip}+\\epsilon _{i}}$\n",
    "\n",
    "![image](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/2560px-Linear_regression.svg.png)\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "![image](https://i.stack.imgur.com/tPhVh.png)\n",
    "\n",
    "![image](https://kasunsviews.files.wordpress.com/2014/10/d8cc6-cvx-fun.gif)\n",
    "\n",
    "![image](https://wngaw.github.io/images/cost_function_3d_plot_2_variables.png)\n",
    "\n",
    "![image](https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/350px-Gradient_descent.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li1oCuBZ7Ck9"
   },
   "source": [
    "## Gradient Method\n",
    "\n",
    "![image](https://miro.medium.com/max/1204/1*voHAOhceZInYOgKIdkAWEA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzT-Z0y57Fky",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Newton's Method\n",
    "\n",
    "![image](https://miro.medium.com/max/684/1*mhXzpBdQsn6wX_1YwC_ZGg.jpeg)\n",
    "\n",
    "![image](https://tutorial.math.lamar.edu/classes/calci/NewtonsMethod_Files/image001.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.4\n",
      "7.12\n",
      "6.096\n",
      "5.2768\n",
      "4.62144\n",
      "4.0971519999999995\n",
      "3.6777215999999995\n",
      "3.3421772799999996\n",
      "3.073741824\n",
      "2.8589934591999997\n"
     ]
    }
   ],
   "source": [
    "a = 0.1\n",
    "xi = 10\n",
    "\n",
    "# for a in alpha:\n",
    "for i in range(10):\n",
    "\n",
    "    diff = 2*(xi-2)*1\n",
    "    xi = xi - a * diff\n",
    "\n",
    "    # print((xi-2)**2 + 5)\n",
    "    print(xi)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r53w6YP07IhR",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Gradient Method\n",
    "\n",
    "\n",
    "The standard (or \"batch\") gradient descent method :\n",
    "\n",
    "${\\displaystyle w:=w-\\eta \\nabla f(w)=w-{\\frac {\\eta }{n}}\\sum _{i=1}^{n}\\nabla f_{i}(w),}$\n",
    "\n",
    "where ${\\displaystyle \\eta }$ is a step size or the learning rate.\n",
    "\n",
    "## Stochastic gradient descent \n",
    "[Reference](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "\n",
    "Choose an initial vector of parameters ${\\displaystyle w}$ and learning rate ${\\displaystyle \\eta }$ .\n",
    "\n",
    "Repeat until an approximate minimum is obtained:\n",
    "\n",
    "-  Randomly shuffle samples in the training set.\n",
    "\n",
    "-  For ${\\displaystyle i=1,2,...,n}$, do:\n",
    "\n",
    "  - ${\\displaystyle w:=w-\\eta \\nabla f_{i}(w).}$\n",
    "\n",
    "![image](https://pythonmachinelearning.pro/wp-content/uploads/2017/09/GD-v-SGD.png)\n",
    "\n",
    "![image](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Stogra.png/440px-Stogra.png)\n",
    "\n",
    "## Momentum\n",
    "\n",
    "![image](https://i.stack.imgur.com/Oqfx5.png)\n",
    "\n",
    "![image](https://eloquentarduino.github.io/wp-content/uploads/2020/04/SGD.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZ8a0hxS7fyJ"
   },
   "source": [
    "## Lagrange Multipliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVwQ3mMY7OX1"
   },
   "source": [
    "## Linear Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JD_OW9N07bbG"
   },
   "source": [
    "## Simplex Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCQ2irbsAltD"
   },
   "source": [
    "## References\n",
    "\n",
    "https://github.com/lilipads/gradient_descent_viz\n",
    "\n",
    "https://ruder.io/optimizing-gradient-descent/index.html#adagrad\n",
    "\n",
    "https://bl.ocks.org/EmilienDupont/aaf429be5705b219aaaf8d691e27ca87\n",
    "\n",
    "https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c\n",
    "\n",
    "http://cvxopt.org/\n",
    "\n",
    "https://distill.pub/2017/momentum/\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Optimization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
